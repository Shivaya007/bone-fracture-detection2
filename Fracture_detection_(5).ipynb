{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f361a7c-65f2-47f6-9966-399faa92edef",
      "metadata": {
        "id": "5f361a7c-65f2-47f6-9966-399faa92edef"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f10b012-e022-4c1e-a13f-5f85425a6589",
      "metadata": {
        "id": "3f10b012-e022-4c1e-a13f-5f85425a6589"
      },
      "outputs": [],
      "source": [
        "import supervision as sv\n",
        "import transformers\n",
        "\n",
        "sv.__version__ , transformers.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49c34a21-66b5-40ef-a827-2009fa7e7ac6",
      "metadata": {
        "id": "49c34a21-66b5-40ef-a827-2009fa7e7ac6"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning\n",
        "print(pytorch_lightning.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04dafe09-6e3b-42a1-8c0e-ad7eb8f66d6c",
      "metadata": {
        "id": "04dafe09-6e3b-42a1-8c0e-ad7eb8f66d6c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torchvision\n",
        "from transformers import DetrImageProcessor\n",
        "image_processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45ad4e83-218e-4125-af50-4aecc412666e",
      "metadata": {
        "id": "45ad4e83-218e-4125-af50-4aecc412666e"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90b6da9f-b35b-4309-8d64-ddf74156c920",
      "metadata": {
        "id": "90b6da9f-b35b-4309-8d64-ddf74156c920"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.listdir('bone fracture.v2-release.coco/valid')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2adb0ec5-7081-4ffb-bda8-a6819f221474",
      "metadata": {
        "id": "2adb0ec5-7081-4ffb-bda8-a6819f221474"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset = 'bone fracture.v2-release.coco'\n",
        "\n",
        "ANNOTATION_FILE_NAME = \"_annotations.coco.json\"\n",
        "TRAIN_DIRECTORY = os.path.join(dataset, \"train\")\n",
        "VAL_DIRECTORY = os.path.join(dataset, \"valid\")\n",
        "TEST_DIRECTORY = os.path.join(dataset, \"test\")\n",
        "\n",
        "\n",
        "class CocoDetection(torchvision.datasets.CocoDetection):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_directory_path: str,\n",
        "        image_processor,\n",
        "        train: bool = True\n",
        "    ):\n",
        "        annotation_file_path = os.path.join(dataset,ANNOTATION_FILE_NAME);\n",
        "        super(CocoDetection, self).__init__(image_directory_path, annotation_file_path)\n",
        "        self.image_processor = image_processor\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        images, annotations = super(CocoDetection, self).__getitem__(idx)\n",
        "        image_id = self.ids[idx]\n",
        "        annotations = {'image_id': image_id, 'annotations': annotations}\n",
        "        encoding = self.image_processor(images=images, annotations=annotations, return_tensors=\"pt\")\n",
        "        pixel_values = encoding[\"pixel_values\"].squeeze()\n",
        "        target = encoding[\"labels\"][0]\n",
        "\n",
        "        return pixel_values, target\n",
        "\n",
        "\n",
        "TRAIN_DATASET = CocoDetection(image_directory_path=TRAIN_DIRECTORY, image_processor=image_processor, train=True)\n",
        "VAL_DATASET = CocoDetection(image_directory_path=VAL_DIRECTORY, image_processor=image_processor, train=False)\n",
        "TEST_DATASET = CocoDetection(image_directory_path=TEST_DIRECTORY, image_processor=image_processor, train=False)\n",
        "\n",
        "print(\"Number of training examples:\", len(TRAIN_DATASET))\n",
        "print(\"Number of validation examples:\", len(VAL_DATASET))\n",
        "print(\"Number of test examples:\", len(TEST_DATASET))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1326f1d-0bd0-4301-bb46-278a3c63453e",
      "metadata": {
        "id": "e1326f1d-0bd0-4301-bb46-278a3c63453e"
      },
      "outputs": [],
      "source": [
        "# Visualize if dataset is loaded properly\n",
        "\n",
        "import random\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# select random image\n",
        "image_ids = TRAIN_DATASET.coco.getImgIds()\n",
        "image_id = random.choice(image_ids)\n",
        "print('Image #{}'.format(image_id))\n",
        "\n",
        "# load image and annotatons\n",
        "image = TRAIN_DATASET.coco.loadImgs(image_id)[0]\n",
        "annotations = TRAIN_DATASET.coco.imgToAnns[image_id]\n",
        "image_path = os.path.join(TRAIN_DATASET.root, image['file_name'])\n",
        "image = cv2.imread(image_path)\n",
        "\n",
        "# annotate\n",
        "detections = sv.Detections.from_coco_annotations(coco_annotation=annotations)\n",
        "\n",
        "# we will use id2label function for training\n",
        "categories = TRAIN_DATASET.coco.cats\n",
        "id2label = {k: v['name'] for k,v in categories.items()}\n",
        "\n",
        "labels = [\n",
        "    f\"{id2label[class_id]}\"\n",
        "    for _, _, class_id, _\n",
        "    in detections\n",
        "]\n",
        "\n",
        "box_annotator = sv.BoxAnnotator()\n",
        "frame = box_annotator.annotate(scene=image, detections=detections, labels=labels)\n",
        "\n",
        "%matplotlib inline\n",
        "sv.show_frame_in_notebook(image, (8, 8))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cac7e82-d70e-4d5b-a6b7-18bada183024",
      "metadata": {
        "id": "5cac7e82-d70e-4d5b-a6b7-18bada183024"
      },
      "outputs": [],
      "source": [
        "# Visualize if dataset is loaded properly\n",
        "\n",
        "import random\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# select random image\n",
        "image_ids = TRAIN_DATASET.coco.getImgIds()\n",
        "image_id = random.choice(image_ids)\n",
        "print('Image #{}'.format(image_id))\n",
        "\n",
        "# load image and annotatons\n",
        "image = TRAIN_DATASET.coco.loadImgs(image_id)[0]\n",
        "annotations = TRAIN_DATASET.coco.imgToAnns[image_id]\n",
        "image_path = os.path.join(TRAIN_DATASET.root, image['file_name'])\n",
        "image = cv2.imread(image_path)\n",
        "\n",
        "# annotate\n",
        "detections = sv.Detections.from_coco_annotations(coco_annotation=annotations)\n",
        "\n",
        "# we will use id2label function for training\n",
        "categories = TRAIN_DATASET.coco.cats\n",
        "id2label = {k: v['name'] for k,v in categories.items()}\n",
        "\n",
        "labels = [\n",
        "    f\"{id2label[class_id]}\"\n",
        "    for _, _, class_id, _\n",
        "    in detections\n",
        "]\n",
        "\n",
        "box_annotator = sv.BoxAnnotator()\n",
        "frame = box_annotator.annotate(scene=image, detections=detections, labels=labels)\n",
        "\n",
        "%matplotlib inline\n",
        "sv.show_frame_in_notebook(image, (8, 8))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4728052a-6d3c-4231-ab49-037b9da4911a",
      "metadata": {
        "id": "4728052a-6d3c-4231-ab49-037b9da4911a"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collate_fn(batch):\n",
        "    pixel_values = [item[0] for item in batch]\n",
        "    encoding = image_processor.pad(pixel_values, return_tensors=\"pt\")\n",
        "    labels = [item[1] for item in batch]\n",
        "    return {\n",
        "        'pixel_values': encoding['pixel_values'],\n",
        "        'pixel_mask': encoding['pixel_mask'],\n",
        "        'labels': labels\n",
        "    }\n",
        "\n",
        "TRAIN_DATALOADER = DataLoader(dataset=TRAIN_DATASET, collate_fn=collate_fn, batch_size=4, shuffle=True)\n",
        "VAL_DATALOADER = DataLoader(dataset=VAL_DATASET, collate_fn=collate_fn, batch_size=4)\n",
        "TEST_DATALOADER = DataLoader(dataset=TEST_DATASET, collate_fn=collate_fn, batch_size=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48172ca2-6dc0-4dc2-8230-78a7a4d83e2b",
      "metadata": {
        "id": "48172ca2-6dc0-4dc2-8230-78a7a4d83e2b"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "from transformers import DetrForObjectDetection\n",
        "import torch\n",
        "\n",
        "\n",
        "class Detr(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, lr, lr_backbone, weight_decay):\n",
        "        super().__init__()\n",
        "        self.model = DetrForObjectDetection.from_pretrained(\n",
        "            pretrained_model_name_or_path=\"facebook/detr-resnet-50\",\n",
        "            num_labels=len(id2label),\n",
        "            ignore_mismatched_sizes=True\n",
        "        )\n",
        "\n",
        "        self.lr = lr\n",
        "        self.lr_backbone = lr_backbone\n",
        "        self.weight_decay = weight_decay\n",
        "\n",
        "    def forward(self, pixel_values, pixel_mask):\n",
        "        return self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
        "\n",
        "    def common_step(self, batch, batch_idx):\n",
        "        pixel_values = batch[\"pixel_values\"]\n",
        "        pixel_mask = batch[\"pixel_mask\"]\n",
        "        labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
        "\n",
        "        outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        loss_dict = outputs.loss_dict\n",
        "\n",
        "        return loss, loss_dict\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, loss_dict = self.common_step(batch, batch_idx)\n",
        "        # logs metrics for each training_step, and the average across the epoch\n",
        "        self.log(\"training_loss\", loss)\n",
        "        for k,v in loss_dict.items():\n",
        "            self.log(\"train_\" + k, v.item())\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss, loss_dict = self.common_step(batch, batch_idx)\n",
        "        self.log(\"validation/loss\", loss)\n",
        "        for k, v in loss_dict.items():\n",
        "            self.log(\"validation_\" + k, v.item())\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # DETR authors decided to use different learning rate for backbone\n",
        "        # you can learn more about it here:\n",
        "        # - https://github.com/facebookresearch/detr/blob/3af9fa878e73b6894ce3596450a8d9b89d918ca9/main.py#L22-L23\n",
        "        # - https://github.com/facebookresearch/detr/blob/3af9fa878e73b6894ce3596450a8d9b89d918ca9/main.py#L131-L139\n",
        "        param_dicts = [\n",
        "            {\n",
        "                \"params\": [p for n, p in self.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
        "            {\n",
        "                \"params\": [p for n, p in self.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
        "                \"lr\": self.lr_backbone,\n",
        "            },\n",
        "        ]\n",
        "        return torch.optim.AdamW(param_dicts, lr=self.lr, weight_decay=self.weight_decay)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return TRAIN_DATALOADER\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3f7754d-60f5-4ecc-9023-958ae341c1d5",
      "metadata": {
        "id": "a3f7754d-60f5-4ecc-9023-958ae341c1d5"
      },
      "outputs": [],
      "source": [
        "model = Detr(lr=1e-4, lr_backbone=1e-5, weight_decay=1e-4)\n",
        "\n",
        "batch = next(iter(TRAIN_DATALOADER))\n",
        "outputs = model(pixel_values=batch['pixel_values'], pixel_mask=batch['pixel_mask'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecfb71e9-a32c-449e-993f-6612b84fcffa",
      "metadata": {
        "id": "ecfb71e9-a32c-449e-993f-6612b84fcffa"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning import Trainer\n",
        "\n",
        "# settings\n",
        "MAX_EPOCHS = 45\n",
        "\n",
        "trainer = Trainer( max_epochs=MAX_EPOCHS, gradient_clip_val=0.1, accumulate_grad_batches=8, log_every_n_steps=5)\n",
        "\n",
        "trainer.fit(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a76833b-fa8b-4404-8007-8cc29f04c6e7",
      "metadata": {
        "id": "3a76833b-fa8b-4404-8007-8cc29f04c6e7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Specify the device\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a52ef2a-29a8-4843-bdf4-0f435211d529",
      "metadata": {
        "id": "1a52ef2a-29a8-4843-bdf4-0f435211d529"
      },
      "outputs": [],
      "source": [
        "MODEL_PATH = 'custom-model'\n",
        "model.model.save_pretrained(MODEL_PATH)\n",
        "\n",
        "# loading model\n",
        "model = DetrForObjectDetection.from_pretrained(MODEL_PATH)\n",
        "model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45a691db-3f62-4620-bb3f-84dd31e955d4",
      "metadata": {
        "id": "45a691db-3f62-4620-bb3f-84dd31e955d4"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CocoDetection(torchvision.datasets.CocoDetection):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_directory_path: str,\n",
        "        image_processor,\n",
        "        train: bool = True\n",
        "    ):\n",
        "        annotation_file_path = os.path.join(dataset,ANNOTATION_FILE_NAME);\n",
        "        super(CocoDetection, self).__init__(image_directory_path, annotation_file_path)\n",
        "        self.image_processor = image_processor\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        images, annotations = super(CocoDetection, self).__getitem__(idx)\n",
        "        image_id = self.ids[idx]\n",
        "        annotations = {'image_id': image_id, 'annotations': annotations}\n",
        "        encoding = self.image_processor(images=images, annotations=annotations, return_tensors=\"pt\")\n",
        "        pixel_values = encoding[\"pixel_values\"].squeeze()\n",
        "        target = encoding[\"labels\"][0]\n",
        "\n",
        "        return pixel_values, target\n",
        "\n",
        "\n",
        "TRAIN_DATASET = CocoDetection(image_directory_path=TRAIN_DIRECTORY, image_processor=image_processor, train=True)\n",
        "VAL_DATASET = CocoDetection(image_directory_path=VAL_DIRECTORY, image_processor=image_processor, train=False)\n",
        "TEST_DATASET = CocoDetection(image_directory_path=TEST_DIRECTORY, image_processor=image_processor, train=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b52aaa65-8de7-4a1d-b819-abe892d8e22c",
      "metadata": {
        "id": "b52aaa65-8de7-4a1d-b819-abe892d8e22c"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# utils\n",
        "categories = TEST_DATASET.coco.cats\n",
        "id2label = {k: v['name'] for k,v in categories.items()}\n",
        "box_annotator = sv.BoxAnnotator()\n",
        "print(categories)\n",
        "# select random image\n",
        "image_ids = TEST_DATASET.coco.getImgIds()\n",
        "print(image_ids)\n",
        "image_id = random.choice(image_ids)\n",
        "print('Image #{}'.format(image_id))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5596dc6a-20a1-445d-bd78-71b324cb518b",
      "metadata": {
        "id": "5596dc6a-20a1-445d-bd78-71b324cb518b"
      },
      "outputs": [],
      "source": [
        "image = TEST_DATASET.coco.loadImgs(image_id)[0]\n",
        "\n",
        "print(image)\n",
        "\n",
        "annotations = TEST_DATASET.coco.imgToAnns[image_id]\n",
        "\n",
        "image_path = os.path.join(TEST_DATASET.root, image['file_name'])\n",
        "\n",
        "image = cv2.imread(image_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65d07bdc-9ba3-44f8-ba05-68f2cf33cced",
      "metadata": {
        "id": "65d07bdc-9ba3-44f8-ba05-68f2cf33cced"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset = 'bone fracture.v2-release.coco'\n",
        "\n",
        "ANNOTATION_FILE_NAME = \"test/_annotations.coco.json\"\n",
        "\n",
        "TEST_DIRECTORY = os.path.join(dataset, \"test\")\n",
        "\n",
        "\n",
        "class CocoDetection(torchvision.datasets.CocoDetection):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_directory_path: str,\n",
        "        image_processor,\n",
        "        train: bool = True\n",
        "    ):\n",
        "        annotation_file_path = os.path.join(dataset,ANNOTATION_FILE_NAME);\n",
        "        super(CocoDetection, self).__init__(image_directory_path, annotation_file_path)\n",
        "        self.image_processor = image_processor\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        images, annotations = super(CocoDetection, self).__getitem__(idx)\n",
        "        image_id = self.ids[idx]\n",
        "        annotations = {'image_id': image_id, 'annotations': annotations}\n",
        "        encoding = self.image_processor(images=images, annotations=annotations, return_tensors=\"pt\")\n",
        "        pixel_values = encoding[\"pixel_values\"].squeeze()\n",
        "        target = encoding[\"labels\"][0]\n",
        "\n",
        "        return pixel_values, target\n",
        "\n",
        "\n",
        "TEST_DATASET = CocoDetection(image_directory_path=TEST_DIRECTORY, image_processor=image_processor, train=False)\n",
        "\n",
        "\n",
        "print(\"Number of test examples:\", len(TEST_DATASET))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c471edcf-308a-4d94-b57e-a0f13744e42c",
      "metadata": {
        "id": "c471edcf-308a-4d94-b57e-a0f13744e42c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Annotate ground truth\n",
        "detections = sv.Detections.from_coco_annotations(coco_annotation=annotations)\n",
        "labels = [f\"{id2label[class_id]}\" for _, _, class_id, _ in detections]\n",
        "print(image)\n",
        "frame_ground_truth = box_annotator.annotate(scene=image.copy(), detections=detections, labels=labels)\n",
        "CONFIDENCE_TRESHOLD = 0.1\n",
        "\n",
        "\n",
        "# Annotate detections\n",
        "with torch.no_grad():\n",
        "\n",
        "    # load image and predict\n",
        "    inputs = image_processor(images=image, return_tensors='pt').to(DEVICE)\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    # post-process\n",
        "    target_sizes = torch.tensor([image.shape[:2]]).to(DEVICE)\n",
        "    results = image_processor.post_process_object_detection(\n",
        "        outputs=outputs,\n",
        "        threshold=CONFIDENCE_TRESHOLD,\n",
        "        target_sizes=target_sizes\n",
        "    )[0]\n",
        "\n",
        "\n",
        "    detections = sv.Detections.from_transformers(transformers_results=results)\n",
        "    labels = [f\"{id2label[class_id]} {confidence:.2f}\" for _, confidence, class_id, _ in detections]\n",
        "    frame_detections = box_annotator.annotate(scene=image.copy(), detections=detections, labels=labels)\n",
        "\n",
        "\n",
        "# %matplotlib inline # Remove this line, we won't use it anymore\n",
        "\n",
        "# Combine both images side by side and display\n",
        "fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
        "axs[0].imshow(cv2.cvtColor(frame_ground_truth, cv2.COLOR_BGR2RGB))\n",
        "axs[0].axis('off')\n",
        "axs[0].set_title('Ground Truth')\n",
        "\n",
        "axs[1].imshow(cv2.cvtColor(frame_detections, cv2.COLOR_BGR2RGB))\n",
        "axs[1].axis('off')\n",
        "axs[1].set_title('Detections')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b57d0ee-0189-4efd-a04b-086812ad6e20",
      "metadata": {
        "id": "7b57d0ee-0189-4efd-a04b-086812ad6e20"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}